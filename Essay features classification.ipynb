{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96c3b5b",
   "metadata": {},
   "source": [
    "# <font color='289C4E'>Part A: Classification<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086a623",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A1.1<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6079b20",
   "metadata": {},
   "source": [
    "Supervised machine learning is a type of machine learning where the algorithm builds a model based off of both input and output data. The algorithm tries to predict the output based off of the input data by either classification or regression. Classification is the idea of predicting a category based off the input data and regression tries to predict a continuous value. Supervised machine learning needs both input and output data so the algorithm knows what the correct answer is or could be given a set of input data. It is different from unsupervised learning which only takes input data and attempts to cluster them into groups.\n",
    "\n",
    "Labelled data consists of values for both input and output variables. The algorithm takes the input data and attempts to map it to the output data (the label) and labelled data gives the algorithm the ground truth or possible correct answer for training data. Labels can either be categorical, such as cats or dogs, or continuous, like a dollar figure.\n",
    "\n",
    "The training and testing datasets are extrapolated from the given dataset. Generally, a machine learning model is trained from a segment of data such as 75% of the data which means the model is given that data to produce a model. The testing data is not given to the algorithm to train with as it is used for validation, it is used to check the accuracy of the model in prediction since we can feed it the input and check its prediction and the real output to see the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57fa8c",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A1.2<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d27f4a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1457</td>\n",
       "      <td>2153</td>\n",
       "      <td>426</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.053991</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>26.625000</td>\n",
       "      <td>423.995272</td>\n",
       "      <td>0.995294</td>\n",
       "      <td>207</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>105</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>424</td>\n",
       "      <td>412</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>503</td>\n",
       "      <td>1480</td>\n",
       "      <td>292</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.068493</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>26.545455</td>\n",
       "      <td>290.993103</td>\n",
       "      <td>0.996552</td>\n",
       "      <td>148</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>77</td>\n",
       "      <td>0.263699</td>\n",
       "      <td>356</td>\n",
       "      <td>345</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253</td>\n",
       "      <td>3964</td>\n",
       "      <td>849</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>4.669022</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>17.326531</td>\n",
       "      <td>843.990544</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>285</td>\n",
       "      <td>0.335689</td>\n",
       "      <td>130</td>\n",
       "      <td>0.153121</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>988</td>\n",
       "      <td>210</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4.704762</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>207.653784</td>\n",
       "      <td>0.988828</td>\n",
       "      <td>112</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>62</td>\n",
       "      <td>0.295238</td>\n",
       "      <td>217</td>\n",
       "      <td>209</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1450</td>\n",
       "      <td>3139</td>\n",
       "      <td>600</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.231667</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>594.652150</td>\n",
       "      <td>0.991087</td>\n",
       "      <td>255</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>165</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>702</td>\n",
       "      <td>677</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essayid  chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0     1457   2153    426      14            6             0         5.053991   \n",
       "1      503   1480    292       9            7             0         5.068493   \n",
       "2      253   3964    849      19           26             1         4.669022   \n",
       "3      107    988    210       8            7             0         4.704762   \n",
       "4     1450   3139    600      13            8             0         5.231667   \n",
       "\n",
       "   sentences  questions  avg_word_sentence         POS  POS/total_words  \\\n",
       "0         16          0          26.625000  423.995272         0.995294   \n",
       "1         11          0          26.545455  290.993103         0.996552   \n",
       "2         49          2          17.326531  843.990544         0.994100   \n",
       "3         12          0          17.500000  207.653784         0.988828   \n",
       "4         24          1          25.000000  594.652150         0.991087   \n",
       "\n",
       "   prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0           207                  0.485915            105   \n",
       "1           148                  0.506849             77   \n",
       "2           285                  0.335689            130   \n",
       "3           112                  0.533333             62   \n",
       "4           255                  0.425000            165   \n",
       "\n",
       "   synonym_words/total_words  unstemmed  stemmed  score  \n",
       "0                   0.246479        424      412      4  \n",
       "1                   0.263699        356      345      4  \n",
       "2                   0.153121        750      750      4  \n",
       "3                   0.295238        217      209      3  \n",
       "4                   0.275000        702      677      4  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import statement and reading file and storing it in variable.\n",
    "# Seeing head of dataset to see format\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('Essay-Features.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ebf7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the independent variables in X and dependent variable (label) in Y\n",
    "X = dataset.iloc[:, 1:17].values # chars, words, commas, apostrophes, punctuations, avg_word_length, sentences, questions, avg_word_sentence, POS, POS/total_words, prompt_words, prompt_words/total_words, synonym_words, synonym_words/total_words, unstemmed, stemmed\n",
    "y = dataset.iloc[:, 18].values # score\n",
    "# Do not take in essayid since the feature will not be used for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33ca81",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A1.3<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ee20a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statement\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Assigning 25% of data to be reserved for testing and using 75% of data for training.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d814437",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A2.1<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abb5a5",
   "metadata": {},
   "source": [
    "Binary classification attempts to categorise the data into two possible classes, it is generally a true or false output to detect the presence or absence of something. Possible examples include spam detection, medical diagnoses and fraud detection. \n",
    "\n",
    "Multi class classification tries to divide the data into multiple (more than two) classes or categories. Examples include image classification, language processing, handwriting recognition and species identification. The algorithm tries to assign each datapoint based off of the input variables to one of the labels (output possibilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a5739",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A2.2a<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797f528",
   "metadata": {},
   "source": [
    "Normalising data, or feature scaling, is used when the raw data has values that vary significantly. So a model that cannot tell the difference between the features will weight the importance of each feature differently. Some objective functions also will not work such as classifiers that calculate the Euclidean distance between two datapoints. If one of the features has very extreme values, it will significantly affect how the distance is calculated, hence why we need to normalise the data so each feature contributes proportionately to the final outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc63204",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A2.2b<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5de502ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# Transforming the explanatory variables so that there are no extreme values so the effect of each variable is proportionate\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef87fa",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A2.3a<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d364ff",
   "metadata": {},
   "source": [
    "Support vector machine is a supervised machine learning algorithm primarily used for classification but can also be used for regression if altered. In generally tries to produce a hyperplane that separates the data into its classes by having a dividing wall between each category of datapoints. \n",
    "\n",
    "In the simplest dimension, a curve of datapoints on a 2 dimensional plane can be sectioned into segments with a linear line, where the middle data is one category and the outside data can be the other in binary classification. When there are more input features, a higher dimension is required which cannot be visualised but the same concept is used to separate the data using planes. \n",
    "\n",
    "In SVM, the data nearest to the plane is called suppor vectors, the algorithm tries to keep the distance between the plane and these support vectors to a maximum to have as good distinct classes as possible to ensure a high accuracy. This is called margin maximisation which the algorithm is designed to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b14b29",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A2.3b<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248da8e1",
   "metadata": {},
   "source": [
    "As stated earlier with the higher dimension for data which has several variables, the kernel aims to transform the data into the higher dimensional space since it may not be possible to separate easily when it is linear. Kernels allow SVM to implicitly attempt to find a hyperplane that segments the data without actually having to calculate the formula for the planes which would take lots of computational power in a higher dimension.\n",
    "\n",
    "Some examples of kernels used are the linear kernel, polynomial, radial basis function and sigmoid kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee8031",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A2.3c<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f53c6782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essayid                      0.033463\n",
       "chars                        0.683983\n",
       "words                        0.662091\n",
       "commas                       0.525055\n",
       "apostrophes                  0.322052\n",
       "punctuations                 0.157976\n",
       "avg_word_length              0.327814\n",
       "sentences                    0.230895\n",
       "questions                    0.277392\n",
       "avg_word_sentence           -0.113036\n",
       "POS                          0.662823\n",
       "POS/total_words              0.311555\n",
       "prompt_words                 0.641119\n",
       "prompt_words/total_words     0.026646\n",
       "synonym_words                0.578352\n",
       "synonym_words/total_words   -0.305405\n",
       "unstemmed                    0.697187\n",
       "stemmed                      0.696776\n",
       "score                        1.000000\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the Pearson's correlation for each feature to score\n",
    "dataset.iloc[:, 0:19].corr(method='pearson')['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6657d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out variables that have low correlation to score such as essayid so that model can be more accurate\n",
    "# essayid is also not needed since it will not tell us anything about the score the student will get, hence remove it\n",
    "X = dataset.iloc[:, [1,2,3,4,6,7,8,9,10,11,13,14,15,16]].values\n",
    "y = dataset.iloc[:, 18].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0b55488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into training and testing data again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "864d2336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling explanatory variables again\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24324fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import code\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07313a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for kernel: linear is 0.6666666666666666\n",
      "Accuracy for kernel: poly is 0.6576576576576577\n",
      "Accuracy for kernel: rbf is 0.6636636636636637\n",
      "Accuracy for kernel: sigmoid is 0.6186186186186187\n"
     ]
    }
   ],
   "source": [
    "# Testing which kernel yields highest accuracy by looping over example kernels and seeing which has highest accuracy\n",
    "# In this case, linear kernel has highest accuracy\n",
    "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    svm_classifier = SVC(kernel = kernel)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    print('Accuracy for kernel:', kernel, 'is', svm_classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34c1840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C: 0.1 is 0.6576576576576577\n",
      "Accuracy for C: 1 is 0.6666666666666666\n",
      "Accuracy for C: 10 is 0.6606606606606606\n",
      "Accuracy for C: 100 is 0.6546546546546547\n"
     ]
    }
   ],
   "source": [
    "# Seeing which value for C will have highest accuracy, this determines whether there is a soft or harder margin for misclassification\n",
    "# Choose C=1\n",
    "for C in [0.1,1,10,100]:\n",
    "    svm_classifier = SVC(kernel = 'linear', C=C)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    print('Accuracy for C:', C, 'is', svm_classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "210672f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for degree: 1 is 0.6546546546546547\n",
      "Accuracy for degree: 2 is 0.5375375375375375\n",
      "Accuracy for degree: 3 is 0.6576576576576577\n",
      "Accuracy for degree: 4 is 0.5675675675675675\n",
      "Accuracy for degree: 5 is 0.6816816816816816\n",
      "Accuracy for degree: 6 is 0.5795795795795796\n",
      "Accuracy for degree: 7 is 0.5765765765765766\n",
      "Accuracy for degree: 8 is 0.5015015015015015\n",
      "Accuracy for degree: 9 is 0.5465465465465466\n",
      "Accuracy for degree: 10 is 0.4804804804804805\n"
     ]
    }
   ],
   "source": [
    "# Seeing which degree of polynomial will yield highest accuracy\n",
    "# Although degree 5 poly is higher than linear, it may be overfitting\n",
    "# Since accuracy increase is not much, I chose to stick with linear\n",
    "for degree in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    svm_classifier = SVC(kernel = 'poly', degree = degree)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    print('Accuracy for degree:', degree, 'is', svm_classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f2a6a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, kernel='linear')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing the model building using linear kernel and C=1\n",
    "svm_classifier = SVC(\n",
    "    kernel = 'linear', \n",
    "    C=1\n",
    ")\n",
    "# Fitting the training data to model\n",
    "svm_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a61b6",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A2.4<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "415d85cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import statement for random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Performing model building using entropy criterion\n",
    "forest_classifier = RandomForestClassifier(\n",
    "    criterion = 'entropy',\n",
    "    random_state = 0\n",
    ")\n",
    "# Fitting the training data to model\n",
    "forest_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc9045",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A3.1<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee27ed3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 3, 2, 3, 4, 4, 4, 2, 4, 4, 3, 4, 3, 3, 4, 3, 4, 4, 4, 3, 3,\n",
       "       3, 3, 3, 4, 4, 4, 3, 3, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 4, 2, 3, 3,\n",
       "       3, 3, 3, 3, 2, 3, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 4, 3, 3, 4, 4,\n",
       "       4, 4, 4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 4, 3, 4, 3, 4, 3, 3, 4, 3, 3,\n",
       "       4, 4, 3, 4, 3, 3, 1, 3, 3, 4, 4, 2, 4, 3, 3, 4, 4, 2, 4, 4, 3, 4,\n",
       "       2, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3, 4, 4, 4, 4, 4,\n",
       "       4, 3, 4, 3, 3, 4, 4, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 4,\n",
       "       4, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 2, 4, 3,\n",
       "       4, 4, 4, 2, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 4, 4,\n",
       "       3, 4, 3, 4, 4, 3, 3, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 3, 3,\n",
       "       4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4,\n",
       "       3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 3, 4, 4, 4, 3, 2, 2, 3, 3, 3, 4, 3,\n",
       "       3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4, 3, 3, 3, 4, 3, 4, 4, 2, 3, 4, 3,\n",
       "       3, 3, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 3, 4,\n",
       "       4, 4, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 4, 4, 4, 4, 3,\n",
       "       4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the results based off of test subset using SVM\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "y_pred_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6c6987f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 3, 2, 3, 4, 4, 4, 2, 4, 4, 3, 4, 3, 3, 4, 3, 4, 4, 4, 3, 3,\n",
       "       3, 3, 3, 4, 4, 4, 3, 4, 4, 3, 3, 4, 4, 2, 4, 4, 3, 3, 4, 2, 4, 3,\n",
       "       3, 3, 3, 3, 2, 3, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 4,\n",
       "       4, 4, 4, 4, 3, 3, 4, 5, 4, 3, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 3,\n",
       "       4, 4, 3, 4, 4, 3, 3, 4, 3, 4, 3, 2, 4, 3, 3, 4, 4, 3, 4, 4, 3, 4,\n",
       "       3, 3, 3, 4, 3, 4, 3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4, 4, 4,\n",
       "       4, 3, 4, 3, 3, 4, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 4,\n",
       "       4, 3, 2, 3, 3, 3, 3, 4, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 4, 3,\n",
       "       4, 4, 4, 2, 3, 4, 4, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 1, 3, 4, 4,\n",
       "       3, 4, 3, 4, 4, 3, 3, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 3, 3,\n",
       "       4, 4, 4, 4, 4, 3, 2, 4, 3, 3, 4, 3, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4,\n",
       "       3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 3, 4, 4, 4, 2, 2, 2, 3, 3, 3, 4, 3,\n",
       "       3, 3, 4, 4, 5, 3, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 4, 2, 3, 4, 3,\n",
       "       3, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4,\n",
       "       4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4, 4, 3,\n",
       "       4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the results based off of test subset using Random forest\n",
    "y_pred_forest = forest_classifier.predict(X_test)\n",
    "y_pred_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4aa48",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A3.2<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd826aec",
   "metadata": {},
   "source": [
    "These 6x6 confusion matrices show which outputs were correctly and incorrectly identified. The diagonals show the number of testing datapoints which are correctly identified. The matrix also shows which were incorrectly identified and what they were identified as. For example the third column shows the actual values that were 3 but what they were predicted as, the second row of that column would mean the number of values the model predicted as 2 but were actually 3. This is why the diagonals show how many were predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83639975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   2,   0,   0,   0,   0],\n",
       "       [  1,   9,  13,   0,   0,   0],\n",
       "       [  0,   2, 108,  37,   0,   0],\n",
       "       [  0,   0,  39, 105,   0,   0],\n",
       "       [  0,   0,   1,  15,   0,   0],\n",
       "       [  0,   0,   0,   1,   0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import statement for cofusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Creating confusion matrix based off test data and predicted outputs\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "cm_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fd85b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   1,   0,   0,   0,   0],\n",
       "       [  0,  11,  11,   1,   0,   0],\n",
       "       [  0,   1, 106,  40,   0,   0],\n",
       "       [  0,   0,  34, 109,   1,   0],\n",
       "       [  0,   0,   1,  14,   1,   0],\n",
       "       [  0,   0,   0,   1,   0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "# Creating confusion matrix based off test data and predicted outputs\n",
    "cm_forest = confusion_matrix(y_test, y_pred_forest)\n",
    "cm_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc338c8d",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A3.3<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba094438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the score of svm for classifiying testing data\n",
    "svm_classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "122f36fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6846846846846847"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the score of random forest model for classifiying testing data\n",
    "forest_classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12b2ea9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# For svm model\n",
    "# Can also be found through confusion matrix\n",
    "# Finds the sum of diagonal and divides by total\n",
    "i = 0\n",
    "correctly_predicted = 0\n",
    "total = 0\n",
    "for row in cm_svm:\n",
    "    # Finding sum of diagonal\n",
    "    correctly_predicted += row[i]\n",
    "    i += 1\n",
    "    # Finding total\n",
    "    for value in row:\n",
    "        total += value\n",
    "print(correctly_predicted/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdf4667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6846846846846847\n"
     ]
    }
   ],
   "source": [
    "# For random forest model\n",
    "# Finds the sum of diagonal and divides by total\n",
    "i = 0\n",
    "correctly_predicted = 0\n",
    "total = 0\n",
    "for row in cm_forest:\n",
    "    # Finding sum of diagonal\n",
    "    correctly_predicted += row[i]\n",
    "    i += 1\n",
    "    # Finding total\n",
    "    for value in row:\n",
    "        total += value\n",
    "print(correctly_predicted/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee75f9",
   "metadata": {},
   "source": [
    "Based on the accuracy of the two models' prediction of the testing data, it is seen that both models are not that great. However it is seen that the random forest performs slightly better since its predictions of the testing data were more accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa782a38",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A4.1<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21d57e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 18)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data and storing, also seeing shape of data to see number of rows.\n",
    "dataset_sample = pd.read_csv('Essay-Features-Submission.csv')\n",
    "dataset_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7124661c",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>A4.2<font><a class='anchor' id='top'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10c41051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 4, 4, 4, 4, 3, 3, 3, 2, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 3, 3,\n",
       "       3, 4, 4, 3, 3, 4, 4, 3, 2, 4, 3, 3, 4, 3, 4, 4, 3, 3, 3, 4, 3, 3,\n",
       "       1, 3, 3, 4, 4, 3, 3, 4, 4, 4, 3, 4, 3, 4, 4, 4, 2, 3, 3, 4, 3, 4,\n",
       "       4, 4, 3, 4, 3, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 3, 4, 2, 4, 4,\n",
       "       2, 3, 3, 3, 4, 3, 4, 3, 4, 4, 4, 3, 3, 5, 2, 3, 3, 4, 3, 3, 4, 4,\n",
       "       4, 3, 3, 4, 4, 3, 2, 4, 2, 3, 3, 4, 4, 3, 3, 4, 3, 4, 3, 2, 3, 4,\n",
       "       2, 3, 4, 4, 3, 3, 1, 4, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3,\n",
       "       4, 3, 3, 3, 4, 4, 4, 3, 2, 3, 4, 4, 3, 3, 2, 3, 3, 4, 3, 4, 4, 4,\n",
       "       4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 4, 3, 4, 3, 4, 4, 3,\n",
       "       3], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting the features used in the model training into the test variable\n",
    "test = dataset_sample.iloc[:,[1,2,3,4,6,7,8,9,10,11,13,14,15,16]].values\n",
    "# Scaling the features the same way we did in model training\n",
    "sc = StandardScaler()\n",
    "test = sc.fit_transform(test)\n",
    "# Predicting using random forest classifier using test data\n",
    "prediction = forest_classifier.predict(test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7090af",
   "metadata": {},
   "source": [
    "Results added to 'A4 predictions.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
